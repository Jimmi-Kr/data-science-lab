---
title: "Module 8: Evaluating Prompt Quality and Output Reliability"
format:
  html:
    toc: true
    toc-depth: 2
    code-fold: true
jupyter: python3
order: 8
---



##  Why Evaluate Prompts?

Evaluating prompts is fundamental to successful prompt engineering. It helps in:

* **Identifying Effective Prompts:** Pinpointing which prompts consistently generate **accurate, helpful,** or **creative** outputs. This is crucial for maximizing the utility of LLMs across diverse applications.
* **Mitigating Undesirable Outputs:** Reducing occurrences of **hallucinations** (LLMs generating false or nonsensical information) and **irrelevant responses** that don't align with the user's intent.
* **Enhancing Generalizability:** Selecting prompts that perform well not just in specific instances but also **generalize better** across a wider range of examples or different users. This ensures robustness and scalability of LLM applications.

---

##  What Makes a “Good” Prompt?

A "good" prompt is one that consistently elicits high-quality responses from an LLM. The characteristics of such outputs typically include:

*  **Relevant** – The output directly addresses and **matches the intent of the user's query**. It doesn't deviate from the core subject or provide extraneous information.
*  **Fluent** – The language used in the output is **grammatically correct, natural-sounding,** and **readable**. It avoids awkward phrasing, typos, or syntactic errors that could hinder understanding.
*  **Coherent** – The ideas presented in the output flow logically and are **well-connected and structured**. There's a clear progression of thought, making the response easy to follow.
*  **Factual** – The output contains **accurate and verifiable information**. This is particularly critical in applications where precision and truthfulness are paramount, such as in factual retrieval or reporting.
*  **Complete** – The output **fully answers the question or task** posed by the prompt, providing all necessary details without leaving out crucial information.

---

##  Human Evaluation Criteria

Human evaluation remains the gold standard for assessing prompt quality, especially for nuanced aspects like creativity, subjectivity, or complex reasoning. A **scoring rubric** is a structured way to conduct this manual assessment.

| Criteria    | Scale (1-5) | Description |
| :---------- | :---------- | :---------- |
| **Relevance** | 1 (poor) – 5 (excellent) | Does the output directly and appropriately match the prompt’s intent? A score of 1 indicates the output is completely off-topic, while 5 means it's perfectly aligned. |
| **Fluency** | 1 – 5       | Is the language natural, grammatically correct, and free from errors? A score of 1 suggests the output is difficult to read due to poor grammar or awkward phrasing, while 5 indicates impeccable language. |
| **Coherence** | 1 – 5       | Are the ideas well-connected, logically organized, and easy to follow? A score of 1 implies a disjointed or confusing output, whereas 5 signifies a highly structured and logical flow. |
| **Factuality** | 1 – 5       | Is the information accurate and verifiable? A score of 1 means the output contains significant factual errors or hallucinations, while 5 denotes complete accuracy. |
| **Completeness**| 1 – 5       | Does the output fully address all aspects of the request or question? A score of 1 indicates a partial or incomplete response, while 5 means the task is entirely fulfilled. |

By using such a rubric, evaluators can provide consistent and comparable scores, allowing for systematic analysis of different prompts and their outputs.

---

##  LLM-as-a-Judge Evaluation

A modern and efficient approach to prompt evaluation involves leveraging another LLM to act as a judge. This method is particularly useful for large-scale evaluations where human review might be impractical.

The core idea is to provide a powerful LLM with:
1.  The original prompt.
2.  Two or more responses generated from different prompts or models.
3.  Instructions for comparing these responses based on specific criteria (e.g., relevance, coherence, conciseness).

```python
prompt = '''
You will be given two responses to the same prompt. Pick the one that is more relevant and coherent.

Prompt: "Explain the concept of climate change."

Response A: "Climate change is the average weather shift over a short time."

Response B: "Climate change refers to long-term alterations in temperature and weather patterns, often due to human activity."

Which is better and why?
'''

from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)


# For demonstration purposes, a placeholder response:
response_content = "Response B is better because it accurately describes climate change as long-term alterations and mentions human activity, which is a key aspect. Response A incorrectly states it's a short-term shift."

print(response_content)
```



#  Advantages of LLM-as-a-Judge

- **Scalability**: Can evaluate a vast number of outputs quickly.
- **Consistency**: LLMs can be more consistent in their judgments than multiple human evaluators, provided the judging prompt is well-defined.
- **Efficiency**: Automates a significant part of the evaluation process.

::: {.callout-caution}
- The judging LLM itself needs to be robust and unbiased to provide reliable assessments.
- The criteria for judgment must be clearly articulated in the judging prompt.
:::



#  A/B Testing of Prompts

Test multiple prompts for the same task.

### Example (Python):

```python

prompt_1 = "Summarize the following article."
prompt_2 = "Give a short and clear summary of this article, limited to 3 sentences."
```

# Evaluate both using human review or LLM
##  Automatic Evaluation Metrics

> Evaluate both using human review or LLM.

###  BLEU Score
Measures n-gram overlap with reference text (used in translation).

###  ROUGE Score
Used for summarization — compares recall of overlapping n-grams.

###  METEOR Score
Similar to BLEU but considers synonyms and stemming.

>  These are more useful when reference outputs are available.

---

::: {.callout-tip}
- Use clear and specific instructions.
- Add examples (few-shot prompting).
- Break complex tasks into smaller steps (chain-of-thought).
- Try different phrasings for better performance.
- Avoid ambiguous or vague language.
:::


#  Example: Evaluating Summary Prompts

### Python Example:

```python
prompt_1 = "Summarize the following in a paragraph."
prompt_2 = "Give a concise, factual summary of this news article in 3 bullet points."
```

::: {.callout-note}

| Problem                      | Solution                |
|-----------------------------|-------------------------|
| Outputs are inconsistent    | Refine prompt           |
| Outputs lack domain knowledge | Fine-tune model      |
| Outputs miss key details    | Add constraints         |
| Outputs hallucinate facts   | Add context or switch model |
:::


###  Hands-On Notebook

<a href="https://colab.research.google.com/drive/1wKVfiR0_FYOQuksPJ7HvjQcOanH56IIJ?usp=drive_link" target="_blank">
  <button style="padding: 10px 20px; background-color: #2e7d32; color: white; border: none; border-radius: 6px; font-size: 16px;">
     Open in Google Colab
  </button>
</a>


 