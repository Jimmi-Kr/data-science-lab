{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Objective  \n- To understand and practice building effective end-to-end machine learning models using this notebook as a companion to the lecture.\n- Some pointers have been provided after various code snippets. These are not specific to this dataset.","metadata":{}},{"cell_type":"markdown","source":"### Instructions\n- Clearly explain how each method or function used in the notebook works.  \n- For every code snippet, document the key insights and takeaways.  ","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:20:05.205217Z","iopub.execute_input":"2025-08-20T06:20:05.205545Z","iopub.status.idle":"2025-08-20T06:20:05.215664Z","shell.execute_reply.started":"2025-08-20T06:20:05.205520Z","shell.execute_reply":"2025-08-20T06:20:05.214578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading the Dataset","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/playground-series-s4e10/sample_submission.csv\")\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s4e10/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s4e10/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:25.729338Z","iopub.execute_input":"2025-08-20T05:13:25.730256Z","iopub.status.idle":"2025-08-20T05:13:26.055592Z","shell.execute_reply.started":"2025-08-20T05:13:25.730183Z","shell.execute_reply":"2025-08-20T05:13:26.054579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pointers\n- We are using `read_csv` from **pandas** to read the file into a DataFrame.  \n- Some formats in which data can be stored include formats such as `.tsv`, `.xlsx`, or in SQL Databases. Look up how to read these file types and try loading files as an exercise.  \n- Sometimes data might have to be converted to a tabular format. As an exercise, practice converting and handling data in **JSON** format as well.  \n","metadata":{}},{"cell_type":"code","source":"print(f'The shape of the training data is {train.shape}')\nprint(f'The shape of the test data is {test.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.057974Z","iopub.execute_input":"2025-08-20T05:13:26.058346Z","iopub.status.idle":"2025-08-20T05:13:26.064025Z","shell.execute_reply.started":"2025-08-20T05:13:26.058320Z","shell.execute_reply":"2025-08-20T05:13:26.063135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the first few rows of the dataset\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.065120Z","iopub.execute_input":"2025-08-20T05:13:26.065417Z","iopub.status.idle":"2025-08-20T05:13:26.111319Z","shell.execute_reply.started":"2025-08-20T05:13:26.065385Z","shell.execute_reply":"2025-08-20T05:13:26.110337Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Description of Features\n1. person_age: The age of the loan applicant in years\n2. person_income: Income of the applicant \n3. person_home_ownership: Status of home ownership among Rent, Own, Mortagage and others   \n4. person_emp_length: Length of employment in years\n5. loan_intent: Purpose of the loan\n6. loan_grade: Some metric assigning a quality score to the loan \n7. loan_amnt: Loan amount requested by the candidate\n8. loan_int_rate: Interest rate associated with the loan\n9. loan_percent_income: Percentage of income to be used for loan payments? \n10. cb_person_default_on_file: Indication of whether the applicant has defaulted earlier\n11. cb_person_cred_hist_length: Length of applicant's credit history in years\n12. loan_status: Approval / Rejection of the loan (Target Variable)","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- Use domain knowledge and prior work for guidance, but avoid letting assumptions overly influence decisions.  \n- Allow the data to reveal its own story instead of creating a story first and forcing the data to fit it.  \n","metadata":{}},{"cell_type":"code","source":"# Information about datatypes and null values in columns\ntrain.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.112217Z","iopub.execute_input":"2025-08-20T05:13:26.112651Z","iopub.status.idle":"2025-08-20T05:13:26.150932Z","shell.execute_reply.started":"2025-08-20T05:13:26.112620Z","shell.execute_reply":"2025-08-20T05:13:26.150002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations:\n- There are no Null values present in any of the columns\n- The categorical columns are **person_home_ownership**, **loan_intent**, **loan_grade**, **cb_person_default_on_file**\n- The numerical columns are **person_age**, **person_income**, **person_emp_length**, **loan_amnt**, **loan_int_rate**, **loan_percent_income**, **cb_person_cred_hist_length**","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- A function may report 0 NULL values, but it typically only checks for `NaN`. Missing values could still be represented in other ways (e.g., blanks, special characters, or placeholders).  \n- Columns may sometimes contain textual representations of numbers (e.g., *one, two, three*) instead of numeric values.  \n","metadata":{}},{"cell_type":"code","source":"# Some statistics about different numerical columns\ntrain.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.151865Z","iopub.execute_input":"2025-08-20T05:13:26.152116Z","iopub.status.idle":"2025-08-20T05:13:26.216974Z","shell.execute_reply.started":"2025-08-20T05:13:26.152096Z","shell.execute_reply":"2025-08-20T05:13:26.215651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations:\n- The columns **person_age** and **person_emp_length** have 123 as the maximum value. These data points are erroneous.\n- Majority of the values for the column **loan_status** appears to be 0. This can indicate imbalance.\n- The columns are in different scales. Note **person_age**, **person_income**, **loan_amnt**, **loan_percent_income**.","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- Can be used to spot the presence of potential outliers\n- Provides an understanding of the **scale** and **distribution** of the data.  \n- Highlights features that may be taking a single constant value. We can remove these","metadata":{}},{"cell_type":"code","source":"#To check for NULL values\ntrain.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.218093Z","iopub.execute_input":"2025-08-20T05:13:26.218439Z","iopub.status.idle":"2025-08-20T05:13:26.251114Z","shell.execute_reply.started":"2025-08-20T05:13:26.218406Z","shell.execute_reply":"2025-08-20T05:13:26.250125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#To check for duplicates\ntrain.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.252111Z","iopub.execute_input":"2025-08-20T05:13:26.252469Z","iopub.status.idle":"2025-08-20T05:13:26.302029Z","shell.execute_reply.started":"2025-08-20T05:13:26.252434Z","shell.execute_reply":"2025-08-20T05:13:26.301092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Number of unique values in each column\ntrain.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.304601Z","iopub.execute_input":"2025-08-20T05:13:26.304876Z","iopub.status.idle":"2025-08-20T05:13:26.335854Z","shell.execute_reply.started":"2025-08-20T05:13:26.304852Z","shell.execute_reply":"2025-08-20T05:13:26.334897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pointers\n- Columns with a number of unique values close to the total number of datapoints (high cardinality) may contribute little value to the model.  \n- The **cardinality** of categorical features helps in deciding the appropriate **encoding technique** to apply.  ","metadata":{}},{"cell_type":"code","source":"#Count of occurence of each value of the feature\ntrain['person_home_ownership'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.336778Z","iopub.execute_input":"2025-08-20T05:13:26.337084Z","iopub.status.idle":"2025-08-20T05:13:26.348514Z","shell.execute_reply.started":"2025-08-20T05:13:26.337060Z","shell.execute_reply":"2025-08-20T05:13:26.347568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pointers\n- Can highlight the **dominant categories** within categorical features.  \n- Can reveal **inconsistencies or typos** in category values (*rent* vs *Rent* vs *RENT*, or *MORTGAGE* vs *MORTGAUGE*).  \n","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nEDA is primarily about exploring the dataset. By examining  individual features or groups of features, we can uncover patterns and insights that guide the decisions made during modeling.","metadata":{}},{"cell_type":"markdown","source":"## The target variable","metadata":{}},{"cell_type":"code","source":"train['loan_status'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.349462Z","iopub.execute_input":"2025-08-20T05:13:26.349805Z","iopub.status.idle":"2025-08-20T05:13:26.369047Z","shell.execute_reply.started":"2025-08-20T05:13:26.349775Z","shell.execute_reply":"2025-08-20T05:13:26.368070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(data=train, x='loan_status')\nplt.title('Distribution of the target variable')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.370106Z","iopub.execute_input":"2025-08-20T05:13:26.370453Z","iopub.status.idle":"2025-08-20T05:13:26.626455Z","shell.execute_reply.started":"2025-08-20T05:13:26.370418Z","shell.execute_reply":"2025-08-20T05:13:26.625565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations\n- The dataset appears to be **imbalanced**\n- Roughly 14% of the values belong to `class 1`","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- Class imbalance in datasets is often domain-specific and should be carefully evaluated.  \n- May require the use of **imbalance handling techniques** (e.g., resampling, synthetic data generation, class weights).  \n- **Accuracy alone is not sufficient** for imbalanced datasets. As an exercise, consider which alternative metrics  would be more appropriate, and why.  \n","metadata":{}},{"cell_type":"markdown","source":"## Univariate Feature Analysis","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nsns.countplot(data=train, x=\"person_home_ownership\", ax=axes[0])\naxes[0].set_title(\"Distribution of Home Ownership\")\n\nsns.countplot(data=train, x=\"loan_grade\", ax=axes[1])\naxes[1].set_title(\"Distribution of Loan Grade\")\n\nsns.countplot(data=train, x=\"loan_intent\", ax=axes[2])\naxes[2].set_title(\"Distribution of Loan Intent\")\nplt.xticks(rotation=45)\nplt.show()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:26.627447Z","iopub.execute_input":"2025-08-20T05:13:26.627785Z","iopub.status.idle":"2025-08-20T05:13:27.230894Z","shell.execute_reply.started":"2025-08-20T05:13:26.627758Z","shell.execute_reply":"2025-08-20T05:13:27.229790Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations\n- Some of the categories are more dominant than the others. Can check the connection between categories and the target\n- If there are unimportant categories, those can be replaced with a new category \"Other\"\n- Does the loan_grade column have a natural ordering for the alphabets?","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.boxplot(data=train['person_age'], ax=axes[0])\naxes[0].set_title(\"Boxplot of Age\")\n\nsns.boxplot(data=train['person_income'], ax=axes[1])\naxes[1].set_title(\"Boxplot of Income\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:27.231882Z","iopub.execute_input":"2025-08-20T05:13:27.232180Z","iopub.status.idle":"2025-08-20T05:13:27.565918Z","shell.execute_reply.started":"2025-08-20T05:13:27.232152Z","shell.execute_reply":"2025-08-20T05:13:27.565043Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations\n- The value > 120 in the column `person_age` could be an erroneous entry. Removal would be a good option\n- In the case of `person_income`, there are some large values but these could be naturally occuring in the dataset. What are some possible approaches to handle this?","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- While claiming points to be outliers (for ex, using a box plot), be clear as to what method is used to label the point as an outlier\n- Different methods may select different points as outliers\n- It is important to distinguish between outlier values that are naturally occuring & those that are erroneous entries\n- In some cases points that are outliers might be the valuable points. For eg: In a *\"Money Transaction\"* dataset, transactions with a very large amount of money could be indicative of *Fraud*","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.boxplot(data=train['loan_amnt'], ax=axes[0])\naxes[0].set_title(\"Boxplot of Loan Amount(Train)\")\n\nsns.boxplot(data=test['loan_amnt'], ax=axes[1])\naxes[1].set_title(\"Boxplot of Loan Amount(Test)\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:27.566739Z","iopub.execute_input":"2025-08-20T05:13:27.566962Z","iopub.status.idle":"2025-08-20T05:13:27.888541Z","shell.execute_reply.started":"2025-08-20T05:13:27.566945Z","shell.execute_reply":"2025-08-20T05:13:27.887733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations:\n- Even though there appear to be outliers in the boxplot of `loan_amnt` for training data, we can retain them as a similar distribution is observed in the case of test data ","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Feature Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nsns.heatmap(train.corr(numeric_only=True), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:27.889561Z","iopub.execute_input":"2025-08-20T05:13:27.889854Z","iopub.status.idle":"2025-08-20T05:13:28.362466Z","shell.execute_reply.started":"2025-08-20T05:13:27.889833Z","shell.execute_reply":"2025-08-20T05:13:28.361664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations:\n- The features `cb_person_cred_hist_length` and `person_age` are highly correlated. This is expected as older people would have a longer credit history\n- The features `loan_int_rate` and `loan_percent_income` have a higher correlation with the target. These have to be looked at more closely","metadata":{}},{"cell_type":"markdown","source":"### Pointers\n- Be clear about what correlation is. What is the range of values? What do those values mean?\n- What happens if two features are highly correlated? How does it impact various models?\n- Are features with strong correlation with the target variable also identified as important features after training a model?","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train, x='loan_grade', hue='loan_status')\nplt.title('Loan Default Rate by Loan Grade')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:28.363461Z","iopub.execute_input":"2025-08-20T05:13:28.364417Z","iopub.status.idle":"2025-08-20T05:13:28.615651Z","shell.execute_reply.started":"2025-08-20T05:13:28.364391Z","shell.execute_reply":"2025-08-20T05:13:28.614856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Initial Observations:\n- The categories D, E, F, G are less frequent than A, B, C. However, in those categories, there are more loans approved than rejected","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=train['loan_status'],y=train['person_income'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:28.616448Z","iopub.execute_input":"2025-08-20T05:13:28.616720Z","iopub.status.idle":"2025-08-20T05:13:28.785796Z","shell.execute_reply.started":"2025-08-20T05:13:28.616701Z","shell.execute_reply":"2025-08-20T05:13:28.784851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations\n- The income of individuals where the loans were approved appear to be under 500,000","metadata":{}},{"cell_type":"code","source":"# subset_features = ['loan_amnt', 'loan_int_rate', 'person_income', 'person_age', 'loan_status']\n# sns.pairplot(train[subset_features], hue='loan_status')\n# plt.title('Pair Plot of Selected Features')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:13:28.786930Z","iopub.execute_input":"2025-08-20T05:13:28.787206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.kdeplot(train[train['loan_status'] == 1]['loan_int_rate'], label='Approved', fill=True)\nsns.kdeplot(train[train['loan_status'] == 0]['loan_int_rate'], label='Non-Approved', fill=True)\nplt.title('CDF of Loan Int Rate by Loan Status')\nplt.xlabel('Loan Int Rate')\nplt.ylabel('Density')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:14:31.281074Z","iopub.execute_input":"2025-08-20T05:14:31.281395Z","iopub.status.idle":"2025-08-20T05:14:31.855689Z","shell.execute_reply.started":"2025-08-20T05:14:31.281367Z","shell.execute_reply":"2025-08-20T05:14:31.854815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initial Observations\n- Loans are more likely to be approved if the interest rates are higher","metadata":{}},{"cell_type":"markdown","source":"# Creating a Validation Set","metadata":{}},{"cell_type":"markdown","source":"## Why do we need a Validation dataset?\n- Train our models to do well on unseen data.\n- Identify good values for the HyperParameters through Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns=[\"id\"])\ntest = test.drop(columns=[\"id\"])\n\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:14:36.708721Z","iopub.execute_input":"2025-08-20T05:14:36.709113Z","iopub.status.idle":"2025-08-20T05:14:36.863394Z","shell.execute_reply.started":"2025-08-20T05:14:36.709082Z","shell.execute_reply":"2025-08-20T05:14:36.862153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pointers\n- The use of stratify can help when there are rare / imbalanced classes\n- Repeatedly using the same validation dataset can indirectly lead to overfitting","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"ordinal_col = [\"loan_grade\"]\nonehot_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\n\nordinal_transformer = OrdinalEncoder(categories=[['G', 'F', 'E', 'D', 'C', 'B', 'A']])  \nonehot_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"ord\", ordinal_transformer, ordinal_col),\n        (\"ohe\", onehot_transformer, onehot_cols)\n    ],\n    remainder=\"passthrough\"  \n)\n\npipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"scaler\", MinMaxScaler())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:14:14.735183Z","iopub.execute_input":"2025-08-20T05:14:14.735465Z","iopub.status.idle":"2025-08-20T05:14:14.741273Z","shell.execute_reply.started":"2025-08-20T05:14:14.735443Z","shell.execute_reply":"2025-08-20T05:14:14.740263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pointers\n- Encoding Categorical variables using One Hot Encoding vs Ordinal vs Target vs Frequency\n- Scaling methods. Is it mandatory to use scaling? Which scaler does well even if there are outliers?\n- How many features are present after preprocessing?\n- Should feature selection methods be used? ","metadata":{}},{"cell_type":"code","source":"X_train_processed = pipeline.fit_transform(X_train)\nX_test_processed = pipeline.transform(X_test)\n\ntest_processed = pipeline.transform(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:14:14.742423Z","iopub.execute_input":"2025-08-20T05:14:14.742775Z","iopub.status.idle":"2025-08-20T05:14:14.943390Z","shell.execute_reply.started":"2025-08-20T05:14:14.742744Z","shell.execute_reply":"2025-08-20T05:14:14.942397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'The shape of the processed training data is {X_train_processed.shape}')\nprint(f'The shape of the processed validation data is {X_test_processed.shape}')\nprint(f'The shape of the processed test data is {test_processed.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:14:54.642054Z","iopub.execute_input":"2025-08-20T05:14:54.642517Z","iopub.status.idle":"2025-08-20T05:14:54.648179Z","shell.execute_reply.started":"2025-08-20T05:14:54.642456Z","shell.execute_reply":"2025-08-20T05:14:54.646819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_processed, y_train)\n\ny_pred_proba = clf.predict_proba(X_test_processed)[:, 1]\n\nroc_auc = roc_auc_score(y_test, y_pred_proba)\nprint(\"ROC AUC Score:\", roc_auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:17:52.007324Z","iopub.execute_input":"2025-08-20T05:17:52.007711Z","iopub.status.idle":"2025-08-20T05:17:53.404567Z","shell.execute_reply.started":"2025-08-20T05:17:52.007686Z","shell.execute_reply":"2025-08-20T05:17:53.403828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploring some basic Models","metadata":{}},{"cell_type":"code","source":"models = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Naive Bayes\": GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"KNN\": KNeighborsClassifier(),\n    \"SVM\": SVC(probability=True, random_state=42),\n    \"Bagging\": BaggingClassifier(random_state=42),\n    \"AdaBoost\": AdaBoostClassifier(random_state=42)\n}\n\ndef evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    \n    results = {}\n    for split, (X, y) in {\"Train\": (X_train, y_train), \"Test\": (X_test, y_test)}.items():\n        y_pred = model.predict(X)\n        y_proba = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X)\n\n        results[split] = {\n            \"Accuracy\": accuracy_score(y, y_pred),\n            \"Precision\": precision_score(y, y_pred, zero_division=0),\n            \"Recall\": recall_score(y, y_pred, zero_division=0),\n            \"F1-Score\": f1_score(y, y_pred, zero_division=0),\n            \"ROC-AUC\": roc_auc_score(y, y_proba)\n        }\n    return results\n\nfinal_results = {}\nfor name, model in models.items():\n    final_results[name] = evaluate_model(model, X_train_processed, y_train, X_test_processed, y_test)\n\nresults_df = pd.concat({outer: pd.DataFrame(inner).T for outer, inner in final_results.items()})\n\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:19:27.341076Z","iopub.execute_input":"2025-08-20T05:19:27.341436Z","iopub.status.idle":"2025-08-20T05:25:35.687582Z","shell.execute_reply.started":"2025-08-20T05:19:27.341409Z","shell.execute_reply":"2025-08-20T05:25:35.686565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\"]\n\nfor metric in metrics:\n    plt.figure(figsize=(10,6))\n    train_scores = [final_results[m][\"Train\"][metric] for m in models.keys()]\n    test_scores = [final_results[m][\"Test\"][metric] for m in models.keys()]\n    \n    x = np.arange(len(models))\n    width = 0.35\n    \n    plt.bar(x - width/2, train_scores, width, label='Train')\n    plt.bar(x + width/2, test_scores, width, label='Test')\n    \n    plt.xticks(x, models.keys(), rotation=45, ha='right')\n    plt.ylabel(metric)\n    plt.title(f\"Model Comparison - {metric}\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:29:59.801951Z","iopub.execute_input":"2025-08-20T05:29:59.802901Z","iopub.status.idle":"2025-08-20T05:30:01.035345Z","shell.execute_reply.started":"2025-08-20T05:29:59.802869Z","shell.execute_reply":"2025-08-20T05:30:01.034299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"rf_params = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\nrf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='f1', n_jobs=-1)\nrf_grid.fit(X_train_processed, y_train)\n\nbest_rf = rf_grid.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:31:30.756910Z","iopub.execute_input":"2025-08-20T05:31:30.757297Z","iopub.status.idle":"2025-08-20T05:39:22.440434Z","shell.execute_reply.started":"2025-08-20T05:31:30.757270Z","shell.execute_reply":"2025-08-20T05:39:22.439613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ada_params = {\n    'n_estimators': [50, 100, 200],\n    'learning_rate': [0.01, 0.1, 1.0]\n}\n\nada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, cv=5, scoring='f1', n_jobs=-1)\nada_grid.fit(X_train_processed, y_train)\n\nbest_ada = ada_grid.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:39:42.485760Z","iopub.execute_input":"2025-08-20T05:39:42.486085Z","iopub.status.idle":"2025-08-20T05:40:59.539959Z","shell.execute_reply.started":"2025-08-20T05:39:42.486060Z","shell.execute_reply":"2025-08-20T05:40:59.539011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef evaluate_best_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n\n    return {\n        \"Accuracy\": accuracy_score(y_test, y_pred),\n        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n        \"F1-Score\": f1_score(y_test, y_pred, zero_division=0),\n        \"ROC-AUC\": roc_auc_score(y_test, y_proba)\n    }\n\nprint(\"Best Random Forest Params:\", rf_grid.best_params_)\nprint(\"Random Forest Test Performance:\", evaluate_best_model(best_rf, X_test_processed, y_test))\n\nprint(\"\\nBest AdaBoost Params:\", ada_grid.best_params_)\nprint(\"AdaBoost Test Performance:\", evaluate_best_model(best_ada, X_test_processed, y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:41:27.394447Z","iopub.execute_input":"2025-08-20T05:41:27.395444Z","iopub.status.idle":"2025-08-20T05:41:28.464242Z","shell.execute_reply.started":"2025-08-20T05:41:27.395396Z","shell.execute_reply":"2025-08-20T05:41:28.463463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='auc')\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 1.0],\n    'colsample_bytree': [0.7, 0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train_processed, y_train)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nbest_model = grid_search.best_estimator_\n\ny_pred = best_model.predict(X_test_processed)\nprint(\"Test Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:48:02.222069Z","iopub.execute_input":"2025-08-20T05:48:02.222458Z","iopub.status.idle":"2025-08-20T05:51:48.968058Z","shell.execute_reply.started":"2025-08-20T05:48:02.222428Z","shell.execute_reply":"2025-08-20T05:51:48.967091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:20:09.858061Z","iopub.execute_input":"2025-08-20T06:20:09.858359Z","iopub.status.idle":"2025-08-20T06:20:10.194102Z","shell.execute_reply.started":"2025-08-20T06:20:09.858331Z","shell.execute_reply":"2025-08-20T06:20:10.193258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:20:29.016705Z","iopub.execute_input":"2025-08-20T06:20:29.017028Z","iopub.status.idle":"2025-08-20T06:20:29.045303Z","shell.execute_reply.started":"2025-08-20T06:20:29.017004Z","shell.execute_reply":"2025-08-20T06:20:29.044438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb.plot_importance(best_model, importance_type=\"gain\", height=0.5, max_num_features=15)\nplt.title(\"Top 15 Feature Importances (Gain)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:20:47.678237Z","iopub.execute_input":"2025-08-20T06:20:47.678566Z","iopub.status.idle":"2025-08-20T06:20:47.967303Z","shell.execute_reply.started":"2025-08-20T06:20:47.678542Z","shell.execute_reply":"2025-08-20T06:20:47.966412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_proba = best_model.predict_proba(X_test_processed)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\nplt.plot([0,1],[0,1],'--',color=\"gray\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:21:26.526000Z","iopub.execute_input":"2025-08-20T06:21:26.526353Z","iopub.status.idle":"2025-08-20T06:21:26.738650Z","shell.execute_reply.started":"2025-08-20T06:21:26.526325Z","shell.execute_reply":"2025-08-20T06:21:26.737739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}}]}